-An operating system is software that manages 
computer hardware and acts as an intermediary between the programs and the hardware.

-Kernel is the core part of the operating 
system that is always running that manages computer hardware and the computer can't run without it.


()()Section Interrupts:

-An interrupt is a mechanism that allows the 
CPU to work in parallel with devices by sending 
it a signal when something important happens. 
The CPU can then pause what it’s doing and handle the event.

- An interrupt is a signal sent to the CPU that tells it to 
pause what it’s doing and handle something important right now.


-When someone presses the "A" key on the keyboard:

1. The **keyboard controller** detects the key press.
2. The controller stores the value of the keystroke in its **buffer**.
3. The controller sends an **interrupt signal** to the CPU.
4. The CPU temporarily **pauses its current task** saves state and calls the **interrupt handler**.
5. The **interrupt handler** invokes the **keyboard driver**.
6. The driver reads the keystroke from the controller's buffer.
7. Finally, the driver sends the keystroke information to the **operating system** for use by programs.

Interrupt Service Routine (ISR) = interrupt handler


(Interrupt Vector Table)
Interrupts need to be handled fast, because they happen very frequently.
A naïve way would be to call a generic routine that looks at what caused the interrupt, then branches to the right handler.
But that’s too slow.

Instead, computers use an Interrupt Vector Table (IVT):
It’s an array stored in low memory.
Each entry is a pointer (address) to the correct ISR for a device.
When an interrupt happens, the hardware gives a unique number (interrupt request number).
That number is used as an index into the table → instantly finds the correct ISR.



The CPU interrupt request line is a physical wire connected to the CPU.
The CPU checks this wire after every instruction. If the line is high, it means there is an interrupt.
The CPU reads the interrupt number from the device.
It then looks up a special table called the interrupt vector.
This table tells the CPU where the code is to handle this specific interrupt, called the interrupt handler.
The interrupt handler saves the CPU’s current state so it can resume later.
It determines what caused the interrupt and performs the required task.
After finishing, the handler restores the CPU’s state.
The CPU then returns to the original task it was executing before the interrupt.
Once the handler is done, the device knows its request has been handled.


what is the above system lacking? 
-1. We need the ability to defer*(not allow) interrupt handling during critical processing.
-2. We need an efficient way to dispatch to the proper interrupt handler for a device *since its a single line
-3. We need multilevel interrupts, so that the operating system can distinguish between high- and low-priority
interrupts and can respond with the appropriate degree of urgency


thats why we use interrupt-controller hardware


- Two Types of CPU Interrupt Lines
-Nonmaskable Interrupt (NMI) Cannot be turned off. Reserved for critical events like unrecoverable memory errors.
CPU must respond immediately.


-Maskable Interrupt (IRQ) Can be temporarily disabled (masked) by the CPU.
Useful when executing critical instructions that must not be interrupted.
Used by most device controllers to request service.


-We have a limitation with the interrupt vector table: there aren’t enough entries to
provide a unique address for every possible device.
The solution is interrupt chaining*important by using a linked list, where a single vector table entry can point to
multiple interrupt handlers linked together.
When an interrupt occurs, the CPU follows the chain, checking each handler in turn until
it finds the one that can service the device.


()()Storage Structure:

the cpu can only load instructions from memory RAM *or DRAM
all programs must be loaded into memeroy to run

RAM is volitile cant store the bootstrap program*the program that starts the operating system so 
computers use EEPROM electrically erasable programmable read-only memory 
EEPROM is infrequently written to and is nonvolatile and slow.

memory provides array of bytes each has its own address
operations like load or store can be done

Program counter is a register inside a cpu that stores 
the memory address for next cpu instruction 
neumann cycle is when cpu stores only one next instruction in the program counter


The CPU loads instructions from memory, using the address stored in the program counter.

Von Neumann Architecture:
CPU, memory, and I/O share a single memory for instructions and data.
CPU follows the fetch–decode–execute cycle:
Fetch instruction from memory (using the Program Counter).
Decode it.
Fetch operands if needed.
Execute.
Store results back.
Memory just sees addresses, not whether they hold instructions or data.


nonvolatile memory (NVM) devices are secondary storage like hdd's or ssd's 


()()I/O Structure:

Interrupt-driven I/O
Works fine for small data.
Problem: For large data (like reading a big file), 
CPU gets too many interrupts (one per byte), 
which slows everything down.

Direct Memory Access (DMA)
Solution for bulk data transfer.
Steps:
CPU sets up buffers, pointers, and counters for the device.
Device controller moves the entire block of data directly between memory and the device.
CPU doesn’t touch each byte, so it’s free to do other tasks.
Only one interrupt is sent when the block is done.



Bus vs Switch architecture
Bus architecture → all components share the same path (bus). Only one thing can communicate at a time.
Switch architecture → multiple components can communicate at the same time, so DMA is even faster here.


()()Single-Processor Systems:

Single-Processor Systems
Definition: A system with one general-purpose CPU, and that CPU has one processing core.
Core: The part of the CPU that actually executes instructions and stores temporary data in registers.


Special-Purpose Processors
These are extra processors built for specific tasks. Examples:
Disk controller CPU (handles disk read/write requests)
Keyboard microprocessor (turns keystrokes into codes)
Graphics controller microprocessor
Characteristics:
They have a limited instruction set (they can’t run general programs or user processes).
Sometimes the OS communicates with them, sending tasks and monitoring status.
Sometimes they are fully autonomous, doing their job without OS intervention.

These special-purpose processors do not make a system multiprocessor because 
the system still has only one general-purpose CPU core.


Multiprocessor Systems

Modern computers often have multiple processors (CPUs).
Each processor may have one core (or multiple cores), and they share resources like:
Bus (communication path)
Sometimes memory, clock, or peripherals

Why use multiple processors?
Goal: Increase throughput → do more work in the same amount of time.
With N processors, you might hope the system is N times faster.
Reality: It’s less than N because:
Overhead – extra work to coordinate tasks between processors.
Resource contention – processors compete for shared memory, buses, or I/O devices.


Clock:
The clock is a timing signal that keeps all parts of the computer in sync.
When multiple processors share a clock, they operate in lockstep timing, which simplifies coordination.

Peripherals:
Peripherals are external devices connected to the computer, like:
Keyboard, mouse, printer, disk drives, network cards


Symmetric Multiprocessing (SMP)

Definition: A multiprocessor system where all CPUs are peers.
Each CPU can run any task, including:
Operating system functions
User programs

Key Features of SMP:
Each CPU has its own registers and local cache
Registers = tiny, super-fast storage inside CPU
Local cache = fast memory near the CPU for frequently used data
All CPUs share main memory
Memory is accessible by all processors through the system bus


Advantages:
Multiple processes can run simultaneously
Example: 2 CPUs → 2 processes run at the same time
Overall performance is higher because tasks can be divided

Challenges / Inefficiencies:
One CPU may be idle while another is overloaded
To avoid this, CPUs should share data structures and dynamically balance the workload
Careful programming is needed to prevent conflicts when multiple CPUs access shared memory


• CPU—The hardware that executes instructions.
• Processor—A physical chip that contains one or more CPUs.
• Core—The basic computation unit of the CPU.
• Multicore— Including multiple computing cores on the same CPU.
• Multiprocessor— Including multiple processors.

Caches in Multicore CPUs
Level 1 (L1) cache: Each core has its own private cache (small but very fast).
Level 2 (L2) cache: Shared by all cores on the chip (larger, slower than L1).
Idea: Combines fast local storage and shared storage for efficiency.

To the operating system, a multicore CPU with N cores looks like N separate CPUs.


Problem with Many CPUs in a Traditional Multiprocessor
Adding more CPUs increases computing power, but only up to a point.
Problem: All CPUs share the system bus, so too many CPUs cause contention → performance can decrease instead of increase.

NUMA (Non-Uniform Memory Access)
Idea: Give each CPU (or group of CPUs) its own local memory accessed via a fast local bus.
CPUs are connected via a shared system interconnect, so all memory is still addressable globally.

Advantages:
Fast local memory access → CPU can read/write its own memory quickly.
No contention for local memory → better scalability with more CPUs.

Drawbacks:
Remote memory access is slower → e.g., CPU0 accessing CPU3’s memory takes longer than accessing its own.
OS must carefully schedule tasks and manage memory to minimize this latency.

Blade Servers
Blade servers are systems where multiple processor boards (blades) share a single chassis.

Each blade:
Boots independently
Runs its own operating system
Some blades are multiprocessor themselves, so a blade server is like multiple independent multiprocessor systems in one chassis.



()()Clustered Systems:
A clustered system = group of independent computers (nodes) connected together,
usually via a LAN (local area network) or faster links like InfiniBand.
Each node is often a multicore system.
They’re used for:

High availability: Service keeps running even if one node fails (through redundancy).
High performance computing: Running big problems in parallel across multiple nodes.


Graceful degradation: Service continues but performance drops when some hardware fails.
Fault tolerance: System keeps running even if one component fails completely.

Asymmetric clustering:
One node works. Another is “hot standby” (idle, only monitors). If main fails → standby takes over.


Symmetric clustering:
All nodes work and monitor each other. More efficient since all hardware is used.


Parallelization: Splitting a program into parts that run on different nodes, then combining results.

Parallel cluster: Multiple nodes share the same storage (requires special software to avoid conflicts).

Storage-Area Network (SAN):
A fast network that lets many systems connect to a shared pool of storage.
If apps + data are stored on the SAN → any node can run them.
If one node fails → another can instantly take over.



()()Operating-System Operations:
Bootstrap program (in firmware) runs first when the computer starts.
Initializes hardware.
Loads the OS kernel into memory.
Kernel starts running and provides system services.
System programs (daemons) start in the background.
On Linux, systemd runs first, then launches other daemons.
Once ready, the system waits for events (work to do).
Events come as interrupts (from hardware) or traps/exceptions (from software).


A system call is a software request for an OS service.


()()Multiprogramming and Multitasking:

Process
-------
A program in execution.
When the OS runs a program, it becomes a process.

Multiprogramming
----------------
Definition: The ability of an operating system to keep multiple processes in memory at the same time, switching the CPU among them.
Goal: Increase CPU utilization (make sure the CPU is never idle if there is work to do).
How it works:
  - The OS loads several processes into memory.
  - One process runs on the CPU.
  - If it must wait (e.g., for I/O), the OS switches the CPU to another process.
  - This cycle continues, so the CPU is always busy.

Multitasking
------------
Definition: A logical extension of multiprogramming, where the CPU switches between processes so quickly that users 
feel multiple programs are running at the same time.

Goal: Provide fast response time to interactive users.
Why needed:
  - User input (keyboard, mouse, touch) is very slow compared to CPU speed.
  - Instead of waiting, the CPU switches to another process, keeping the system responsive.

CPU Scheduling
--------------
Definition: The method the OS uses to decide which process runs next when multiple are ready.
Importance: Ensures fairness, efficiency, and good response times.

Memory Management
-----------------
Definition: Managing how multiple processes share memory safely and efficiently.
Challenge: Several processes must reside in memory simultaneously.
Virtual Memory:
  - Allows a process to run even if it is not completely in physical memory.
  - Lets programs be larger than the actual RAM.
  - Gives the illusion of a large, continuous memory space.



File System
-----------
Definition: The part of the OS that organizes and manages files stored on secondary storage (e.g., hard disk, SSD).



Process Synchronization and Communication
-----------------------------------------
Definition: Mechanisms to ensure processes can work together without conflicts.

Deadlock
--------
Definition: A situation where processes are stuck waiting for each other, and none can continue.
Example: Process A waits for Process B’s resource, while Process B waits for Process A’s resource.
OS must prevent or resolve deadlocks 

Protection
----------
Definition: Preventing processes from interfering with each other or with the OS itself.
Example: One process cannot read another’s memory without permission.



()()Dual-Mode and Multimode Operation:


Dual-Mode Operation
-------------------
Purpose: Protects the operating system (OS) and user programs from interfering with each other.

How: The CPU operates in two modes:
  - User Mode – for running user applications; limited privileges.
  - Kernel Mode (Supervisor/System Mode) – for running OS code; full privileges.

Mode Bit: A hardware bit that indicates the current mode (0 = kernel, 1 = user).

Example:
  - When a program requests an OS service (via a system call), 
    the CPU switches from user mode → kernel mode to safely execute privileged operations.

Privileged Instructions
-----------------------
Certain instructions (like I/O control, timer management, or switching to kernel mode) can only be executed in kernel mode.
Attempting them in user mode triggers a trap to the OS, preventing harm.

Multimode / Multiple Rings
--------------------------
Modern CPUs may have more than two modes:
  - Intel: 4 rings (Ring 0 = kernel, Ring 3 = user, rings 1–2 rarely used)
  - ARMv8: 7 modes
  - Some systems have a virtualization mode for virtual machine managers (VMMs) with privileges between kernel and user.

Instruction Execution Lifecycle
-------------------------------
  - Control starts in kernel mode at boot.
  - OS loads a user application → switches to user mode.
  - When the application needs OS services → triggers a system call, trap, or interrupt → switches back to kernel mode.
  - OS executes the requested service and returns control to the program in user mode.

System Calls
------------
Definition: Mechanism for a user process to request OS services.

Process:
  - User program executes a system call (usually via a trap or syscall instruction).
  - CPU switches to kernel mode.
  - OS verifies parameters, executes the service, and returns control to the program.

Error Handling / Protection
---------------------------
  - Hardware detects illegal operations (e.g., executing privileged instructions in user mode or accessing forbidden memory).
  - Violations trigger a trap to the OS, which can:
      - Terminate the program abnormally.
      - Provide an error message.
      - Optionally dump memory for debugging.


Timer
-------------------------
Purpose: Ensures the OS keeps control of the CPU and prevents user programs from running forever.

How it works:
  - OS sets a timer to generate an interrupt after a fixed or variable period.
  - Variable timers use a counter decremented by a clock; when it reaches 0 → interrupt occurs.

Example:
  - A 10-bit counter with 1 ms ticks allows interrupts from 1–1024 ms.

Behavior:
  - On interrupt, control returns to the OS; the OS can stop the program or give it more time.

Privileged Instruction:
  - Only the OS can set or modify the timer.

()()Resource Management:


Process Management 
---------------------------------------

Process
-------
A process is a program in execution, the basic unit of work in a system.
Examples: A compiler running on a PC, a web browser displaying a page, or a social media app on a mobile device.

Difference from a program:
  - Program = passive entity (stored on disk).
  - Process = active entity (executing on CPU).

Resources Needed by a Process
-----------------------------
A process requires resources to execute:
  - CPU time – to execute instructions
  - Memory – for code, data, and stack
  - Files – input/output or temporary storage
  - I/O devices – e.g., screen, keyboard, network
  - Processes may also receive initialization data (input).
When a process finishes, the OS reclaims its resources for reuse.

Single-threaded vs Multithreaded Processes
------------------------------------------
Single-threaded process:
  - Has one program counter pointing to the next instruction.
  - Executes instructions sequentially.
  - Only one instruction executes at a time per process.

Multithreaded process:
  - Contains multiple program counters (threads), each executing independently.
  - Allows concurrent execution within the same process.

System Processes vs User Processes
----------------------------------
System processes: Execute operating system code.
User processes: Execute user applications.
Both types may execute concurrently on a single CPU (via time-sharing) or in parallel on multiple CPU cores.

OS Responsibilities for Process Management
------------------------------------------
  - Creating and deleting processes (user and system).
  - Scheduling processes and threads on CPU(s).
  - Suspending and resuming processes.
  - Providing process synchronization mechanisms.
  - Providing process communication mechanisms.


()()Memory Management:
-------------------------------------------

Main Memory
-----------
Main memory is a large array of bytes, each with a unique address.
It is the CPU’s directly accessible storage, used for both instructions and data.
Example: To process data from disk, the CPU must first load it into main memory.

Memory and Program Execution
----------------------------
A program must be mapped to absolute addresses and loaded into memory before execution.
While executing, the program accesses instructions and data from memory using these addresses.
When the program finishes, its memory space is freed for other programs.

Need for Memory Management
--------------------------
To improve CPU utilization and system responsiveness, multiple programs are often kept in memory simultaneously (multiprogramming).
This creates a need for the OS to manage memory efficiently.

OS Responsibilities in Memory Management
----------------------------------------
  - Tracking memory usage – which parts of memory are used and by which process.
  - Allocating and deallocating memory – giving memory to processes when needed and reclaiming it when done.
  - Deciding which data/processes to move – swapping processes or data into and out of memory to optimize performance.

Techniques
----------
Many memory-management schemes exist, each requiring different hardware support.
The choice of algorithm depends on the hardware design and system requirements.



()()Mass-Storage Management:
--------------------------------------------

Secondary Storage
-----------------
Secondary storage (e.g., HDDs, SSDs/NVMs) backs up main memory and stores programs and data permanently.
Programs like compilers, browsers, or games are stored here until loaded into main memory.
The OS manages secondary storage efficiently because system performance depends on it.

OS Responsibilities for Secondary Storage
-----------------------------------------
  - Mounting and unmounting – making storage devices accessible or removing them safely.
  - Free-space management – tracking which storage areas are available.
  - Storage allocation – assigning space to programs and files.
  - Disk scheduling – deciding the order in which read/write requests are handled.
  - Partitioning – dividing storage into logical sections.
  - Protection – preventing unauthorized access or misuse of data.

Tertiary Storage
----------------
Slower, cheaper storage with large capacity (e.g., magnetic tapes, CDs, DVDs, Blu-ray).
Used for backups, archival, or rarely used data.

OS may help manage tertiary storage by:
  - Mounting/unmounting media
  - Allocating devices to processes
  - Migrating data from secondary to tertiary storage
